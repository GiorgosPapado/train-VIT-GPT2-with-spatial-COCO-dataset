{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor,AutoTokenizer\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"PYDEVD_WARN_SLOW_RESOLVE_TIMEOUT\"] = \"3.0\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.ln_cross_attn.bias', 'h.9.crossattention.c_proj.bias', 'h.4.crossattention.q_attn.bias', 'h.3.crossattention.c_attn.bias', 'h.4.ln_cross_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.11.crossattention.c_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.11.ln_cross_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.1.ln_cross_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.5.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.6.ln_cross_attn.weight', 'h.10.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.bias', 'h.11.ln_cross_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.ln_cross_attn.bias', 'h.4.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.2.ln_cross_attn.bias', 'h.2.crossattention.c_proj.bias', 'h.9.crossattention.c_attn.weight', 'h.0.ln_cross_attn.weight', 'h.8.ln_cross_attn.bias', 'h.5.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.bias', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.6.crossattention.c_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.8.ln_cross_attn.weight', 'h.3.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.10.ln_cross_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.9.ln_cross_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.bias', 'h.7.ln_cross_attn.weight', 'h.5.ln_cross_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.1.crossattention.c_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.weight', 'h.8.crossattention.c_proj.weight', 'h.6.crossattention.c_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.5.crossattention.c_proj.bias', 'h.8.crossattention.q_attn.bias', 'h.5.crossattention.c_attn.bias', 'h.2.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.weight', 'h.3.ln_cross_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.4.ln_cross_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.10.ln_cross_attn.weight', 'h.9.ln_cross_attn.bias', 'h.0.crossattention.c_proj.bias', 'h.5.crossattention.q_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.7.ln_cross_attn.bias', 'h.6.crossattention.c_proj.bias', 'h.4.crossattention.c_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor, AutoImageProcessor\n",
    "\n",
    "image_encoder_model = \"Centaur31/vit-base\"\n",
    "text_decode_model = \"gpt2\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    image_encoder_model, text_decode_model)\n",
    "\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# image feature extractor\n",
    "feature_extractor = AutoImageProcessor.from_pretrained(image_encoder_model)\n",
    "# text tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_decode_model)\n",
    "\n",
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# update the model config\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.encoder.embeddings.patch_embeddings.projection\n",
    "\n",
    "output_dir = \"vit-gpt-model\"\n",
    "model.save_pretrained(output_dir)\n",
    "feature_extractor.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "# df = pd.read_csv('train_dataset.csv')\n",
    "# dataset_dict = df.to_dict(orient='list')\n",
    "# dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# val_df = pd.read_csv('val_dataset.csv')\n",
    "# val_dataset_dict = val_df.to_dict(orient='list')\n",
    "# val_dataset = Dataset.from_dict(val_dataset_dict)\n",
    "#val_dataset = val_dataset.remove_columns(\"Unnamed: 0\")\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class CustomTensorDataset(Dataset):\n",
    "#     def __init__(self, file_paths):\n",
    "#         self.file_paths = file_paths\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.file_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Load the tensor from file using streaming or any other technique\n",
    "#         tensor = torch.load(self.file_paths[idx])\n",
    "#         return tensor\n",
    "    \n",
    "# # Define file paths for your training and validation tensors\n",
    "# train_file_paths = ['full_training_tensor.pt']\n",
    "# val_file_paths = ['f16val_tensor_data.pt']\n",
    "\n",
    "# # Create instances of the CustomTensorDataset class\n",
    "# train_dataset = CustomTensorDataset(train_file_paths)\n",
    "# val_dataset = CustomTensorDataset(val_file_paths)\n",
    "\n",
    "# # Use DataLoader for efficient streaming and batching\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "# def feature_extraction_fn(image_file):\n",
    "# # Define your image_paths and feature_extractor\n",
    "#     with Image.open(image_file) as img:\n",
    "#         image_path = feature_extractor(img, return_tensors=\"np\")\n",
    "#     return image_path.pixel_values  \n",
    "\n",
    "# def feature_extraction_fn(image_files):\n",
    "#     # Assuming feature_extractor is defined somewhere\n",
    "#     # Make sure it can handle batch processing\n",
    "#     with Image.open(image_files) as img:\n",
    "#         # If using a Hugging Face feature extractor\n",
    "#         images = [feature_extractor(img, return_tensors=\"np\")]\n",
    "#     return images\n",
    "#     return tokenized_labels\n",
    "def feature_extraction_fn(image_paths):\n",
    "    images = []\n",
    "    for image_file in tqdm(image_paths, desc=\"Loading Images\"):\n",
    "        try:\n",
    "            with Image.open(image_file) as file:\n",
    "                img = file.convert(\"RGB\")\n",
    "                images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image: {image_file}\")\n",
    "            print(f\"Error details: {e}\")\n",
    "\n",
    "    if not images:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    # Rest of your code\n",
    "    encoder_inputs = feature_extractor(images=images, return_tensors=\"np\")\n",
    "    return encoder_inputs.pixel_values\n",
    "\n",
    "\n",
    "def tokenization_fn(captions, max_target_length):\n",
    "    \"\"\"Run tokenization on captions.\"\"\"\n",
    "    labels = tokenizer(captions, \n",
    "                      padding=\"max_length\", \n",
    "                      max_length=max_target_length).input_ids\n",
    "\n",
    "    return labels\n",
    "\n",
    "def preprocess_fn(examples, max_target_length):\n",
    "    \"\"\"Run tokenization + image feature extraction\"\"\"\n",
    "    image_paths = examples['image_path']\n",
    "    captions = examples['caption']\n",
    "    model_inputs = {}\n",
    "    # This contains image path column\n",
    "    model_inputs['labels'] = tokenization_fn(captions, max_target_length)\n",
    "    #model_inputs['pixel_values'] = feature_extraction_fn(image_paths)\n",
    "    model_inputs['pixel_values'] = feature_extraction_fn(image_paths)#torch.load(tensor)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_and_append(example):\n",
    "#     # Assuming local_tensor is a PyTorch tensor\n",
    "#     model_inputs = {}\n",
    "#     model_inputs['labels'] = tokenization_fn(example['caption'], 80)  \n",
    "#     model_inputs['pixel_values'] = feature_extraction_fn(example['image_path'])#specific_tensor[image_id] \n",
    "#     #inputs = Dataset.from_dict(model_inputs) \n",
    "#     return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_dataset.csv' with the actual name of your CSV file\n",
    "val_file_path = 'val_dataset.csv'\n",
    "file_path = 'train_dataset.csv'\n",
    "# Load the dataset from the CSV file into a pandas DataFrame\n",
    "val_df = pd.read_csv(val_file_path)\n",
    "df = pd.read_csv(file_path)\n",
    "# Display the first few rows of the DataFrame to inspect the loaded data\n",
    "dataset = df.to_dict(orient='records')\n",
    "val_dataset = val_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "column_names = ['image', 'caption', 'image_path', 'height', 'width', 'image_id', 'file_name']\n",
    "columns = {key: [item[key] for item in dataset] for key in column_names}\n",
    "\n",
    "# Convert the dictionary to a Hugging Face Dataset\n",
    "processed_dataset = Dataset.from_dict(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "column_names = ['image', 'caption', 'image_path', 'height', 'width', 'image_id', 'file_name']\n",
    "columns = {key: [item[key] for item in val_dataset] for key in column_names}\n",
    "\n",
    "# Convert the dictionary to a Hugging Face Dataset\n",
    "val_processed_dataset = Dataset.from_dict(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'caption', 'image_path', 'height', 'width', 'image_id', 'file_name'],\n",
      "    num_rows: 64172\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "filtered_dataset = processed_dataset.filter(lambda example: example['caption'] and example['caption'].strip() != \".\")\n",
    "\n",
    "# Print the filtered dataset\n",
    "print(filtered_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'caption', 'image_path', 'height', 'width', 'image_id', 'file_name'],\n",
      "    num_rows: 31156\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "val_filtered_dataset = val_processed_dataset.filter(lambda example: example['caption'] and example['caption'].strip() != \".\")\n",
    "\n",
    "# Print the filtered dataset\n",
    "print(val_filtered_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "midpoint = len(filtered_dataset) // 2\n",
    "\n",
    "# Split the dataset into two parts\n",
    "train_dataset_part1 = filtered_dataset.select(list(range(midpoint)))\n",
    "train_dataset_part2 = filtered_dataset.select(list(range(midpoint, len(filtered_dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/32086 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Images: 100%|██████████| 1000/1000 [00:14<00:00, 68.20it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 142.17it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 137.85it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 147.19it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 131.65it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 146.36it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 147.84it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 140.13it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 139.91it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 149.77it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 149.29it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 142.60it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 138.38it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 148.34it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 139.67it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 153.86it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 152.74it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 151.06it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 158.86it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 150.26it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 131.71it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 141.00it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 132.27it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 146.87it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 147.85it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 140.87it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 137.77it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 134.75it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 143.78it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 147.18it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 137.60it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 149.97it/s]\n",
      "Loading Images: 100%|██████████| 86/86 [00:00<00:00, 150.80it/s]]\n",
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "mapped_dataset = train_dataset_part2.map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"max_target_length\": 200},\n",
    "    #remove_columns=processed_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# from tqdm import tqdm\n",
    "# #= ['image', 'caption', 'image_path', 'height', 'width', 'image_id', 'file_name']\n",
    "# def convert_to_csv(record):\n",
    "#     # Your conversion logic here\n",
    "#     return {'image_path': record['image_path'], 'caption': record['caption'], 'labels': record['labels'], 'pixel_values': record['pixel_values']}  # Modify this according to your needs\n",
    "\n",
    "# def convert_dataset_to_csv_parallel(mapped_dataset, csv_file, num_workers=4):\n",
    "#     with open(csv_file, 'w', newline='') as csvfile:\n",
    "#         fieldnames = ['image_path', 'caption', 'labels', 'pixel_values']  # Replace with your actual field names\n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "#         # Write header\n",
    "#         writer.writeheader()\n",
    "\n",
    "#         # Use ProcessPoolExecutor for parallel processing\n",
    "#         with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "#             futures = []\n",
    "            \n",
    "#             # Iterate over the dataset and submit tasks for parallel processing\n",
    "#             for record in mapped_dataset:\n",
    "#                 futures.append(executor.submit(convert_to_csv, record))\n",
    "            \n",
    "#             # tqdm is used to display a progress bar\n",
    "#             for future in tqdm(executor.as_completed(futures), total=len(futures), desc=\"Converting\"):\n",
    "#                 result = future.result()\n",
    "#                 writer.writerow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "midpoint = len(val_filtered_dataset) // 2\n",
    "\n",
    "# Split the dataset into two parts\n",
    "val_dataset_part1 = val_filtered_dataset.select(list(range(midpoint)))\n",
    "val_dataset_part2 = val_filtered_dataset.select(list(range(midpoint, len(val_filtered_dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/15578 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Images: 100%|██████████| 1000/1000 [00:07<00:00, 140.12it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 146.87it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 153.55it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 164.40it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 146.96it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 146.03it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 159.94it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:06<00:00, 163.26it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:04<00:00, 206.25it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:01<00:00, 625.09it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:00<00:00, 2443.57it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:00<00:00, 2457.14it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:00<00:00, 2477.03it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:00<00:00, 2459.67it/s]\n",
      "Loading Images: 100%|██████████| 1000/1000 [00:00<00:00, 2418.36it/s]\n",
      "Loading Images: 100%|██████████| 578/578 [00:00<00:00, 2295.54it/s]\n",
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "val_mapped_dataset = val_dataset_part1.map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"max_target_length\": 200},\n",
    "    #remove_columns=val_dataset_part1.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_captions = []\n",
    "\n",
    "# # Iterate through each example in val_dataset\n",
    "# for example in tqdm(dataset, desc=\"Tokenizing Captions\"):\n",
    "#     # Access the 'caption' field of the example\n",
    "#     caption = example['caption']\n",
    "#     # Tokenize the caption and append to the list\n",
    "#     tokens = tokenization_fn(caption, max_target_length=150)\n",
    "#     tokenized_captions.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, entry in enumerate(dataset):\n",
    "#     # Add the 'labels' column to each entry\n",
    "#     entry['labels'] = tokenized_captions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #VALIDATION\n",
    "# tokenized_captions = []\n",
    "\n",
    "# # Iterate through each example in val_dataset\n",
    "# for example in tqdm(val_dataset, desc=\"Tokenizing Captions\"):\n",
    "#     # Access the 'caption' field of the example\n",
    "#     caption = example['caption']\n",
    "#     # Tokenize the caption and append to the list\n",
    "#     tokens = tokenization_fn(caption, max_target_length=150)\n",
    "#     tokenized_captions.append(tokens)\n",
    "\n",
    "# # Add the 'labels' column to the dataset\n",
    "# len(tokenized_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, entry in enumerate(val_dataset):\n",
    "#     # Add the 'labels' column to each entry\n",
    "#     entry['labels'] = tokenized_captions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "\n",
    "# column_names = ['image', 'caption', 'image_path', 'height', 'width', 'image_id', 'file_name', 'pixel_values', 'labels']\n",
    "# columns = {key: [item[key] for item in dataset] for key in column_names}\n",
    "\n",
    "# # Convert the dictionary to a Hugging Face Dataset\n",
    "# processed_dataset = Dataset.from_dict(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "\n",
    "# column_names = ['image', 'caption', 'image_path', 'height', 'width', 'image_id', 'file_name', 'pixel_values', 'labels']\n",
    "# columns = {key: [item[key] for item in val_dataset] for key in column_names}\n",
    "\n",
    "# # Convert the dictionary to a Hugging Face Dataset\n",
    "# val_processed_dataset = Dataset.from_dict(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset, DatasetDict\n",
    "# dataset_dict = DatasetDict({\n",
    "#     \"train\": processed_dataset,\n",
    "#     \"validation\": val_processed_dataset\n",
    "# })\n",
    "# dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    "    output_dir=\"./image-captioning-output-111epochs\",\n",
    "    resume_from_checkpoint='/data1/ViTgpt2/image-captioning-output-111epochs/checkpoint-12500', \n",
    "    num_train_epochs= 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ignore_pad_token_for_loss = True\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "                                                     decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds,\n",
    "                            references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import pyarrow as pa\n",
    "\n",
    "# # Convert your Hugging Face dataset to a Pandas DataFrame\n",
    "# df = pd.DataFrame(mapped_dataset)\n",
    "\n",
    "# # Convert the Pandas DataFrame to a Feather file\n",
    "# feather_file = 'mapped_train.feather'\n",
    "# df.to_feather(feather_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import pyarrow as pa\n",
    "\n",
    "# # Convert your Hugging Face dataset to a Pandas DataFrame\n",
    "# val_df = pd.DataFrame(val_mapped_dataset)\n",
    "\n",
    "# # Convert the Pandas DataFrame to a Feather file\n",
    "# feather_file = 'mapped_val.feather'\n",
    "# val_df.to_feather(feather_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=mapped_dataset,\n",
    "    #eval_dataset=val_updated_dataset,\n",
    "    eval_dataset=val_mapped_dataset,\n",
    "    #train_dataset=processed_dataset['train'],\n",
    "    #eval_dataset=processed_dataset['validation'],\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.compute_metrics(eval_preds)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/13380 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  2%|▏         | 297/13380 [19:07<13:59:54,  3.85s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1650\u001b[0m )\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/transformers/trainer.py:1916\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1913\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1915\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1916\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1917\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1918\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/transformers/data/data_collator.py:70\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[1;32m     71\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/transformers/data/data_collator.py:136\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    134\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mstack([f[k] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features]))\n\u001b[1;32m    135\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([f[k] \u001b[39mfor\u001b[39;49;00m f \u001b[39min\u001b[39;49;00m features])\n\u001b[1;32m    138\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./image-captioning-output-111epochs/tokenizer_config.json',\n",
       " './image-captioning-output-111epochs/special_tokens_map.json',\n",
       " './image-captioning-output-111epochs/vocab.json',\n",
       " './image-captioning-output-111epochs/merges.txt',\n",
       " './image-captioning-output-111epochs/added_tokens.json',\n",
       " './image-captioning-output-111epochs/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./image-captioning-output-111epochs\")\n",
    "\n",
    "tokenizer.save_pretrained(\"./image-captioning-output-111epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "image_captioner = pipeline(\"image-to-text\", model=\"./image-captioning-output-111epochs\", max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption = \"/data1/ViTgpt2/test_images/COCO_test2015_000000000076.jpg\"\n",
    "# image_captioner(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"/data1/ViTgpt2/test_images\"\n",
    "filenames = [os.path.join(test_path, f) for f in os.listdir(test_path) if os.path.isfile(os.path.join(test_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_up_to_n_sentences(captions, n):\n",
    "    for caption in captions:\n",
    "        generated_text = caption.get('generated_text', '')\n",
    "        sentences = generated_text.split('.')\n",
    "        result = '.'.join(sentences[:n])\n",
    "        #print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filenames:\n",
    "    generated_captions = image_captioner(filename)\n",
    "    print_up_to_n_sentences(generated_captions, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_numbers(text_descriptions):\n",
    "    clean_text_descriptions = []\n",
    "    for line in text_descriptions:\n",
    "        clean_text_descriptions.append((re.sub(r'\\d+','', line))[1:])\n",
    "    return clean_text_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files_path = \"/data1/ViTgpt2/test_images_text\"\n",
    "text_files = [file for file in os.listdir(text_files_path) if file.endswith('_desc.txt')]\n",
    "text_files = sorted(text_files)\n",
    "all_descriptions = []\n",
    "#text_files = sorted(text_files[:100])\n",
    "# Create a dictionary to store text descriptions with image filenames (without extension) as keys\n",
    "\n",
    "# Load text descriptions from each text file and match them with the images\n",
    "for text_file in text_files:\n",
    "    text_file_path = os.path.join(text_files_path, text_file)\n",
    "\n",
    "    with open(text_file_path, 'r') as file:\n",
    "        text_descriptions = file.read().splitlines()\n",
    "\n",
    "    text_descriptions = remove_numbers(text_descriptions)\n",
    "    all_descriptions.append(text_descriptions)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(sentences_list_1, sentences_list_2):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    # Function to calculate cosine similarity\n",
    "    def calculate_similarity(sentence1, sentence2):\n",
    "        vectorizer = CountVectorizer().fit_transform([sentence1, sentence2])\n",
    "        vectors = vectorizer.toarray()\n",
    "        return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
    "\n",
    "    # Iterate through sentences and calculate similarity\n",
    "    for sentence_2 in sentences_list_2[0].split('.'):\n",
    "        if sentence_2.startswith('The'):\n",
    "            similarity_scores = []\n",
    "            for sentence_1 in sentences_list_1:\n",
    "                if sentence_1.startswith('The'):\n",
    "                    similarity = calculate_similarity(sentence_1, sentence_2)\n",
    "                    similarity_scores.append((sentence_1, similarity))\n",
    "\n",
    "            # Sort similarity scores in descending order\n",
    "            similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Print the top similarity score\n",
    "            if similarity_scores:\n",
    "                print(f\"For sentence: '{sentence_2}'\")\n",
    "                print(f\"Most similar sentence: '{similarity_scores[0][0]}' with similarity score: {similarity_scores[0][1]:.2%}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sentence: 'The truck is in front of the car'\n",
      "Most similar sentence: 'The car is in front of the truck' with similarity score: 100.00%\n",
      "\n",
      "None\n",
      "For sentence: 'The person is in front of the motorcycle'\n",
      "Most similar sentence: 'The person is in front of the motorcycle' with similarity score: 100.00%\n",
      "\n",
      "None\n",
      "For sentence: 'The person is to the right of the person'\n",
      "Most similar sentence: 'The person is to the right of the person' with similarity score: 100.00%\n",
      "\n",
      "None\n",
      "For sentence: 'The cow is to the right of the cow'\n",
      "Most similar sentence: 'The cow is behind the cow' with similarity score: 84.37%\n",
      "\n",
      "None\n",
      "For sentence: 'The car is in front of the cat'\n",
      "Most similar sentence: 'The car is in front of the car' with similarity score: 91.29%\n",
      "\n",
      "None\n",
      "None\n",
      "For sentence: 'The dining table is in front of the bowl'\n",
      "Most similar sentence: 'The dining table is in front of the bowl' with similarity score: 100.00%\n",
      "\n",
      "None\n",
      "None\n",
      "For sentence: 'The person is to the right of the tennis racket'\n",
      "Most similar sentence: 'The tennis racket is to the right of the sports ball' with similarity score: 90.95%\n",
      "\n",
      "None\n",
      "For sentence: 'The person is to the right of the person'\n",
      "Most similar sentence: 'The person is to the right of the person' with similarity score: 100.00%\n",
      "\n",
      "None\n",
      "For sentence: 'The person is in front of the person'\n",
      "Most similar sentence: 'The person is in front of the person' with similarity score: 100.00%\n",
      "\n",
      "None\n",
      "For sentence: 'The person is to the right of the person'\n",
      "Most similar sentence: 'The person is to the right of the person' with similarity score: 100.00%\n",
      "\n",
      "None\n",
      "For sentence: 'The zebra is to the right of the sheep'\n",
      "Most similar sentence: 'The zebra is to the right of the zebra' with similarity score: 93.93%\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for i, filename in enumerate(filenames):\n",
    "    generated_captions = image_captioner(filename)\n",
    "    x = all_descriptions[i]\n",
    "    y = print_up_to_n_sentences(generated_captions, 3)\n",
    "    y = y.split('. ')\n",
    "    print(similarity(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The zebra is to the right of the sheep',\n",
       " 'The sheep is to the left of the zebra',\n",
       " 'The zebra is in front of the sheep']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
